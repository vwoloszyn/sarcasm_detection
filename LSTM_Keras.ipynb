{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class(x):\n",
    "    #print x['tweet;class']\n",
    "    classe = x['tweet;class'].split(';')[1]\n",
    "    return classe\n",
    "\n",
    "def get_text(x):\n",
    "    #print x['tweet;class']\n",
    "    text = x['tweet;class'].split(';')[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'strange', 1.4054651081081644), (u'nice', 1.4054651081081644)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import islice\n",
    "import operator\n",
    "\n",
    "def get_most_relevant(a,b,n):\n",
    "    s=[]\n",
    "    corpus = [a,b]\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    idf = vectorizer.idf_\n",
    "    x= dict(zip(vectorizer.get_feature_names(), idf))\n",
    "    \n",
    "    sorted_dic = sorted(x.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    elements=sorted_dic[:n]\n",
    "    for w,tf in elements:\n",
    "        s.append(w)\n",
    "    return elements\n",
    "        \n",
    "    \n",
    "a= \"This is very strange\"\n",
    "b=\"This is very nice\"\n",
    "get_most_relevant(a,b,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'#', 4273)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11803 unique words tokens.\n",
      "Using vocabulary size 10000.\n",
      "The least frequent word in our vocabulary is 'sarahwbal' and appeared 1 times.\n",
      "\n",
      "Example sentence: '  - enough said!! woiii!! '\n",
      "\n",
      "Example sentence after Pre-processing: '[u'-', u'enough', u'said', u'!', u'!', 'UNKNOWN_TOKEN', u'!', u'!']'\n"
     ]
    }
   ],
   "source": [
    "percent_train=0.9\n",
    "vocabulary_size = 10000\n",
    "max_review_length = 30\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "df = pd.read_csv('olympic_2012_pos_neg.csv')\n",
    "df['class']= df.apply(get_class,axis=1)\n",
    "df['text']= df.apply(get_text,axis=1)\n",
    "\n",
    "sentences=[]\n",
    "classes=[]\n",
    "lens=[]\n",
    "for r in df.T.to_dict().values():\n",
    "    text = (str(r['text']).decode('iso-8859-1').lower())\n",
    "    sentences.append(text)\n",
    "    lens.append(len(text.split()))\n",
    "    classes.append(r['class'])\n",
    "\n",
    "#max_review_length = int(np.mean(lens))\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())    \n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "\n",
    "##vocab by tfidf\n",
    "#vocab_p=\"\"\n",
    "#vocab_n=\"\"\n",
    "#for i in xrange(len(classes)):\n",
    "#    if classes[i]== \"positive\":\n",
    "#        vocab_p = vocab_p + \" \" + sentences[i]\n",
    "#    else:\n",
    "#        vocab_n = vocab_n + \" \" + sentences[i]#\n",
    "#\n",
    "#vocab= get_most_relevant(vocab_p,vocab_n, vocabulary_size-1)\n",
    "\n",
    "\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " \n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
    " \n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    " \n",
    "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]\n",
    "\n",
    "Y=[]\n",
    "for i in xrange(len(classes)):\n",
    "    if classes[i]==\"positive\":\n",
    "        Y.append(1)\n",
    "    else:\n",
    "         Y.append(0)\n",
    "            \n",
    "            \n",
    "# Create the training data\n",
    "sidx = np.random.permutation(len(tokenized_sentences))\n",
    "n_train = int(np.round(len(tokenized_sentences) * (percent_train)))\n",
    "valid_set_x = [tokenized_sentences[s] for s in sidx[n_train:]]\n",
    "y_valid = [Y[s] for s in sidx[n_train:]]\n",
    "train_set_x = [tokenized_sentences[s] for s in sidx[:n_train]]\n",
    "y_train = [Y[s] for s in sidx[:n_train]]\n",
    "    \n",
    "    \n",
    "\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in train_set_x])\n",
    "X_valid = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in valid_set_x])\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_valid = sequence.pad_sequences(X_valid, maxlen=max_review_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 30, 32)        320000      embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 100)           53200       embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             101         lstm_4[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 373301\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/6\n",
      "3632/3632 [==============================] - 5s - loss: 0.5763 - acc: 0.7373     \n",
      "Epoch 2/6\n",
      "3632/3632 [==============================] - 5s - loss: 0.3491 - acc: 0.8527     \n",
      "Epoch 3/6\n",
      "3632/3632 [==============================] - 4s - loss: 0.1123 - acc: 0.9639     \n",
      "Epoch 4/6\n",
      "3632/3632 [==============================] - 4s - loss: 0.0392 - acc: 0.9893     \n",
      "Epoch 5/6\n",
      "3632/3632 [==============================] - 4s - loss: 0.0172 - acc: 0.9956     \n",
      "Epoch 6/6\n",
      "3632/3632 [==============================] - 5s - loss: 0.0096 - acc: 0.9978     \n",
      "Accuracy: 94.55%\n"
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, nb_epoch=6, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "#x= np.asarray([[word_to_index[w] for w in \"not bad\".split()]])\n",
    "#xpad=sequence.pad_sequences(x, maxlen=max_review_length)\n",
    "#model.predict(xpad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
